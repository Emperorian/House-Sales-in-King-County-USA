# House-Sales-in-King-County-USA

## Table Of Contents

- [ABOUT THE PROJECT](#about-the-project)
- [DATASET SOURCE](#dataset-source)
- [REQUIRED LIBRARIES](#required-libraries)
- [DATA WRANGLING](#data-wrangling)
- [EXPLORATORY DATA ANALYSIS](#exploratory-data-analysis)
- [MODEL DEVELOPMENT](#model-development)
- [MODEL EVALUATION AND REFINEMENT](#model-evaluation-and-refinement)
- [CONCLUSION](#conclusion)




### ABOUT THE PROJECT
   In this project, As a Data Analyst working at a Real Estate Investment Trust. The Trust would like to start investing in Residential real estate. The task is to determine the market price of a house given a set of features. Also analyze and predict housing prices using attributes or features such as square footage, number of bedrooms, number of floors, and so on. 


### DATASET SOURCE
The dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015. It was taken from [Here](https://www.kaggle.com/harlfoxem/housesalesprediction?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-wwwcourseraorg-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDA0101ENSkillsNetwork20235326-2022-01-01)
It was also slightly modified for this project. For this project, I utilized JupyterLab running on the Cloud in the Skills Network Labs environment. 
**Notebook URL:** Alternatively, you can work on your local machine or any other environment of choice, by downloading this link:[Notebook link House Sales](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/FinalModule_Coursera/House_Sales_in_King_Count_USA.ipynb)    

 
#### **Variable**	            **Description**
- id	                   - A notation for a house
- date	                 -Date house was sold
- price	                -Price is the prediction target
- bedrooms       	      -Number of bedrooms
- bathrooms      	      -Number of bathrooms
- sqft_living	          -Square footage of the home
- sqft_lot             	-Square footage of the lot
- floors	               -Total floors (levels) in the house
- waterfront	           -House which has a view of the waterfront
- View	                 -Has been viewed
- condition	            -How good the condition is overall
- grade	overall         -grade given to the housing unit, based on the King County grading system
- sqft_above	           -Square footage of the house apart from the basement
- sqft_basement	        -Square footage of the basement
- yr_built	             -Built Year
- yr_renovated	         -Year when the house was renovated
- zipcode	              -Zip code
- lat	                  -Latitude coordinate
- long	                 -Longitude coordinate
- sqft_living15	        -Living room area in 2015(implies-- some renovations) This might or might not have affected the lot size area
- sqft_lot15	           -LotSize area in 2015(implies-- some renovations)


### REQUIRED LIBRARIES
For this project, all processes will be carried out using Python. All Libraries required for this procedure are listed below. The libraries pre-installed on Skills Network Labs are commented on.
!mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 sci-kit-learn==0.20.1  Note: If your environment doesn't support "!mamba install", use "!pip install" you can also a Jupiter notebook or any ide that support python. 

For suppress warnings: it is important to add this for caution, This will suppress all warnings generated by your code. However, it's generally not recommended to ignore warnings without understanding their implications. In some cases, warnings may indicate potential issues in your code or data that should be addressed. Use caution when suppressing warnings and ensure that it's done intentionally and not as a blanket approach.
```
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
```
now it is time to add the required library for the importation and process of the dataset, this library will also be throughout the project but in the course of the process other required libraries will be added will necessary. in furtherance let's add the following:
```
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LinearRegression
```


### IMPORTING THE DATA
Importing will be impossible without the libraries listed above, in particular pandas, which is the origin of read files however, This version of the lab is working on JupyterLite, which requires the dataset to be downloaded to the interface. While working on the downloaded version of this notebook on their local machines(Jupyter Anaconda), you can simply skip the steps below, and simply use the URL directly in the pandas.read_csv() function. You can uncomment and run the statements in the cell below.

```
import piplite
await piplite.install('seaborn')
```
```
from pyodide.http import pyfetch

async def download(url, filename):
    response = await pyfetch(url)
    if response.status == 200:
        with open(filename, "wb") as f:
            f.write(await response.bytes())
```
```
filepath='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/FinalModule_Coursera/data/kc_house_data_NaN.csv'
```
```
await download(filepath, "housing.csv")
file_name="housing.csv"
```
The read function will load the dataset into the interface
```
df = pd.read_csv(file_name)
```
Don't forget the file path still reminds
```
#filepath='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/FinalModule_Coursera/data/kc_house_data_NaN.csv'
#df = pd.read_csv(filepath, header=None)
```
for us to have a glimpse of the table we use 
```
df.head()
```
We use the method head to display the first 5 columns of the data frame.
![image](https://github.com/Emperorian/House-Sales-in-King-County-USA/assets/101293550/0a03a85b-6c2f-4f72-bfd1-870b5fa0f95b)

**Data types**
Data types are crucial in programming and data analysis as they define the nature and characteristics of the data being manipulated. They determine how data is stored in memory, how it can be accessed, and what operations can be performed on it. Choosing appropriate data types ensures efficient memory usage and computational performance while maintaining data integrity. For instance, using numerical data types like integers or floats for numerical calculations ensures precision and avoids rounding errors, while categorical data types help in organizing and analyzing qualitative data efficiently. Additionally, understanding data types is essential for data validation, transformation, and compatibility, enabling seamless integration and interoperability across different systems and platforms. Overall, data types play a fundamental role in shaping data representation, processing, and analysis, thus underpinning the foundation of reliable and insightful decision-making in various domains.
```
df.dtypes
```
![image](https://github.com/Emperorian/House-Sales-in-King-County-USA/assets/101293550/7d5f450f-94f8-4f33-b3bd-84f420f2c644)

we can further get insight by using the describe function, this will give us a broader statistical summary of the data frame of the set. The describe function() is 
```
df.describe()
```
this above function will a table 
![image](https://github.com/Emperorian/House-Sales-in-King-County-USA/assets/101293550/a34d6a66-16d0-4b8e-a856-94cb6d2afb5f)




### DATA WRANGLING
Data wrangling encompasses the process of transforming and cleaning raw data into a structured and usable format for analysis, visualization, and modeling. It involves tasks such as data collection, data cleaning (handling missing values, outliers, and inconsistencies), data transformation (reshaping, aggregating, merging), and data enrichment (feature engineering, deriving new variables). Effective data wrangling is essential for ensuring data quality, consistency, and reliability, which are prerequisites for making informed decisions and deriving meaningful insights. It often requires a combination of domain knowledge, statistical techniques, and computational tools to handle diverse data formats, large volumes, and complex relationships within the data. Ultimately, proficient data wrangling facilitates the extraction of actionable insights and drives value creation from data across various domains and industries. 
In this case, the data is not as dirty as some data can be. some one or two things need to be done to make the data perfect for use. First, from the table above, we are going to Drop the columns "id" and "Unnamed: 0" from axis 1 using the method drop(), then use the method describe() to obtain a statistical summary of the data and Make sure the in place parameter is set to True.

```
df.drop(['id', 'Unnamed: 0'], axis=1, inplace=True)
df.head()
```
![image](https://github.com/Emperorian/House-Sales-in-King-County-USA/assets/101293550/e2e7dd3a-facd-461b-b538-6131d6592245)

then, We can see we have missing values for the columns  in bedrooms and  bathrooms to fix this 
```
print("number of NaN values for the column bedrooms :", df['bedrooms'].isnull().sum())
print("number of NaN values for the column bathrooms :", df['bathrooms'].isnull().sum())
```
number of NaN values for the column bedrooms: 13 number of NaN values for the column bathrooms: 10. We can replace the missing values of the column 'bedrooms' with the mean of the column 'bedrooms'  using the method replace(). Don't forget to set the inplace parameter to True

```
mean=df['bedrooms'].mean()
df['bedrooms'].replace(np.nan,mean, inplace=True)
```
We also replace the missing values of the column 'bathrooms' with the mean of the column 'bathrooms'  using the method replace(). Don't forget to set the  inplace  parameter to  True 

```
mean=df['bathrooms'].mean()
df['bathrooms'].replace(np.nan,mean, inplace=True)
```
```
print("number of NaN values for the column bedrooms :", df['bedrooms'].isnull().sum())
print("number of NaN values for the column bathrooms :", df['bathrooms'].isnull().sum())
```
The result will be, the number of NaN values for the column bedrooms : 0, number of NaN values for the column bathrooms : 0



### EXPLORATORY DATA ANALYSIS
Exploratory Data Analysis (EDA) is a critical initial step in data analysis where the main focus is on summarizing and visualizing the main characteristics of the dataset to better understand its structure, patterns, and relationships between variables. EDA involves techniques such as summary statistics, data visualization (histograms, scatter plots, box plots), and correlation analysis to gain insights into the distribution, central tendency, dispersion, and potential outliers in the data. It helps in identifying trends, anomalies, and potential patterns that can guide further analysis and hypothesis generation. EDA is crucial for informing subsequent modeling decisions, feature selection, and refining research questions, ultimately leading to more robust and meaningful conclusions from the data.
For the exploratory stage, the count is our first point of call, Use the method value_counts to count the number of houses with unique floor values, and use the method .to_frame() to convert it to a data frame. this is the value count in the series

```
df['floors'].value_counts
```
![image](https://github.com/Emperorian/House-Sales-in-King-County-USA/assets/101293550/08ff58a8-0416-45ec-80b0-3c377fb2744f)
we can now convert the series into a data frame
![image](https://github.com/Emperorian/House-Sales-in-King-County-USA/assets/101293550/9c8f89e5-8780-4bbd-9fa6-7493f421a53a)
some numeric presentations can be very clumsy to understand even by bright but visualization makes it easier at first glance. To further buttress the exploratory analysis, a boxplot will be used for clarity. Use the function boxplot in the seaborn library to determine whether houses with a waterfront view or without a waterfront view have more price outliers comparing the price and waterfront. 

```
sns.boxplot(x="waterfront", y="price", data=df)
```
![image](https://github.com/Emperorian/House-Sales-in-King-County-USA/assets/101293550/1bef6ce0-983a-471f-8793-f45b2557747d)

the visualization clearly shows the effect of the price on houses with a waterfront and the ones without. showing houses with waterfront 1 and without o and a lot of outliers too. regression plot is another way to further deepen the exploratory. replot is a function provided by the Seaborn library in Python for creating scatter plots with linear regression fits. It is a part of the Seaborn Library's high-level interface for drawing attractive and informative statistical graphics. in this case, the function regplot in the seaborn library will be used to determine if the feature sqft_above is negatively or positively correlated with the price
 ```
sns.regplot(x="sqft_above", y="price", data=df, line_kws={"color": "red"})
plt.ylim(0,)
```
![image](https://github.com/Emperorian/House-Sales-in-King-County-USA/assets/101293550/e3f7685a-3b5f-4658-a5de-217a839a260c)

finally, we use the correlation. To find the feature other than 'price' that is most correlated with 'price' using the corr() method in pandas, you can compute the correlation coefficients between 'price' and all other features, and then select the one with the highest absolute correlation coefficient. 
```
df.corr()['price'].sort_values()
```
![image](https://github.com/Emperorian/House-Sales-in-King-County-USA/assets/101293550/beddbd14-8c83-4211-9b0e-5205ff44355e)




### MODEL DEVELOPMENT
Model development is a crucial stage in data analysis and machine learning where predictive or descriptive models are created based on the insights gained from data exploration and preprocessing. This process typically involves selecting an appropriate model algorithm, splitting the dataset into training and testing sets, training the model on the training set, evaluating its performance on the testing set, and iteratively refining the model to improve its predictive accuracy or descriptive power. Various techniques such as cross-validation, hyperparameter tuning, feature selection, and model evaluation metrics are employed to ensure the model's robustness and generalization ability. The ultimate goal of model development is to build a reliable and effective model that can make accurate predictions or provide valuable insights for decision-making in real-world scenarios. Since model development is about finding the best fit, it will be done with a linear regression. We can Fit a linear regression model using the longitude feature 'long' and calculate the R^2.
```
X = df[['long']]
Y = df['price']
lm = LinearRegression()
lm.fit(X,Y)
lm.score(X, Y)
```
which gave 0.00046769430149007363
another combination with price to fit the best is sqft living,  Fit a linear regression model to predict the 'price' using the feature 'sqft_living' should give
```
x = df[['sqft_living']]
y = df['price']
lm = LinearRegression()
lm.fit(x,y)
lm.score(x,y)
```
reg: 0.4928532179037931

to further get a fitting a multiple linear model estimator can be used, we can extract the multiple predictor variables and store them in the variable z. Fit a linear regression model to predict the 'price' using the list of features: 

```
features =["floors", "waterfront","lat" ,"bedrooms" ,"sqft_basement" ,"view" ,"bathrooms","sqft_living15","sqft_above","grade","sqft_living"]     
```
then train the model 
```

Z = df[["floors", "waterfront", "lat", "bedrooms", "sqft_basement", "view", "bathrooms", "sqft_living15", "sqft_above", "grade", "sqft_living"]]
Y = df['price']

lm = LinearRegression()  # Initiate the LinearRegression class
lm.fit(Z, Y)

print(lm.score(Z, Y))
```
model: 0.6576890354915759



### MODEL EVALUATION AND REFINEMENT
Model evaluation and refinement are critical stages in the machine learning workflow, aimed at assessing the performance of developed models and enhancing their predictive accuracy or descriptive power. This iterative process involves selecting appropriate evaluation metrics, such as accuracy or mean squared error, to quantify the model's performance, and employing techniques like cross-validation to estimate its generalization ability. Hyperparameter tuning, feature engineering, and ensemble methods are then utilized to optimize the model's performance and improve its ability to capture underlying patterns in the data. Moreover, ensuring model interpretability and validation on a separate dataset are essential steps to guarantee its effectiveness in real-world applications. Through this iterative refinement process, practitioners can iteratively enhance the model's robustness and reliability, leading to more accurate predictions or insightful conclusions for decision-making purposes. To get first the necessary libraries or modules need to be imported 
```
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
print("done")
```
We will split the data into training and testing sets:

```
features =["floors", "waterfront","lat" ,"bedrooms" ,"sqft_basement" ,"view" ,"bathrooms","sqft_living15","sqft_above","grade","sqft_living"]    
X = df[features]
Y = df['price']

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=1)


print("number of test samples:", x_test.shape[0])
print("number of training samples:",x_train.shape[0])
```
which will give us: the number of test samples: 3242
number of training samples: 18371

now we create and fit a Ridge regression object using the training data, set the regularization parameter to 0.1, and calculate the R^2 using the test data, we will to bring in ridge
```
from sklearn.linear_model import Ridge
```
```
RidgeModel=Ridge(alpha=0.1)
RidgeModel.fit(x_train, y_train)
yhat = RidgeModel.predict(x_test)
print(r2_score(y_test,yhat))
```
ridge regression is 0.647875916393907

finally, Performance of a second-order polynomial transform on both the training data and testing data. then Create and fit a Ridge regression object using the training data, set the regularisation parameter to 0.1, and calculate the R^2 utilizing the test data provided.

```
pr = PolynomialFeatures(degree=2)
x_train_pr = pr.fit_transform(x_train)
x_test_pr = pr.fit_transform(x_test)
RidgeModel.fit(x_train_pr, y_train)
y_hat = RidgeModel.predict(x_test_pr)
print(r2_score(y_test,y_hat))
```
polynomial transformation: 0.7002744263583341




### CONCLUSION
Features: These are the characteristics or attributes of the houses that we are using to predict their prices. For example, the number of floors, whether there's a waterfront, the latitude of the property, the number of bedrooms, etc.

X and Y: These are the datasets we use for training our prediction model. X contains the features (like number of floors, etc.), and Y contains the actual prices of the houses.

Train-test split: We split our dataset into two parts: training data and testing data. The training data (x_train, y_train) is used to teach our model to predict house prices. The testing data (x_test, y_test) is used to see how well our model performs on new, unseen data.

Number of samples: After splitting, we see that our testing dataset contains 3242 houses, and our training dataset contains 18371 houses. This means we're using a larger portion of the data (about 85%) for training the model, which helps it to learn better.

 The number 0.7002744263583341 is the R-squared value, which is a measure of how well the polynomial regression model predicts the house prices for the dataset. In simpler terms, an R-squared value of 0.70 means that around 70% of the variability in house prices can be explained by the features used in the model, such as the number of floors, whether the property has waterfront access, its location (latitude), number of bedrooms, etc.














